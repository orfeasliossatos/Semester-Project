{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e2d833",
   "metadata": {},
   "source": [
    "# Sample Complexity Gap\n",
    "\n",
    "This notebook aims to demonstrate the stated sample complexity gap in **Why Are Convolutional Networks More Sample Efficient Than Fully-Connected Nets? by Zhiyuan Li, Yi Zhang and Sanjeev Arora** [1]. We set up an experiment in which we should see the gap as an increasing polynomial curve of degree less than two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafc9178",
   "metadata": {},
   "source": [
    "## 1. Methods\n",
    "\n",
    "For a given input dimension $d$, we seek the number $|S_{tr}|$ of training samples needed for a model to reach $\\epsilon=0.9$ test accuracy. Then we plot the difference of training samples needed between a Convolutional Neural Network and a Fully Connected Neural Network for increasing values of $d$.\n",
    "\n",
    "### Data\n",
    "\n",
    "The inputs are $3\\times k \\times k$ RGB images for $k\\in \\mathbb{N}$, yielding input dimensions $d\\in \\{3,12,27,48,75,108,147,243,300,...\\}$. We create full training set of $10'000$ images and a test set of $10'000$ and we ask \"the first *how-many* training samples are needed to reach $90\\%$ test accuracy if we train until convergence\"? The training sets are constructed in the following manner.\n",
    "+ Entry-wise independent Gaussian (mean 0, standard deviation 1)\n",
    "\n",
    "We explore two different labelling functions \n",
    "\\begin{equation}\n",
    "h_1=\\mathbb{1}[\\sum_{i\\in R} x_i > \\sum_{i \\in G}x_i] \\quad\\mathrm{ and }\\quad h_2=\\mathbb{1}[\\sum_{i\\in R} x_i^2 > \\sum_{i \\in G}x_i^2].\n",
    "\\end{equation}\n",
    "\n",
    "### Models\n",
    "\n",
    "1. 2-layer CNN.\n",
    "    + Convolution - one kernel per input channel of size 3x3, 10 output channels, stride size 1, and padding of 1, and bias\n",
    "    + Activation function\n",
    "    + Pooling: Max pooling, kernel size 2x2, stride 2\n",
    "    + Flattening\n",
    "    + Fully connected layer (? in, 1 out) with bias\n",
    "    + Sigmoid\n",
    "2. 2-layer FCNN \n",
    "    + Fully connected layer (192 in, 3072 out) with bias\n",
    "    + Activation function \n",
    "    + Fully connected layer (3072 in, 1 out) with bias\n",
    "    + Sigmoid\n",
    "    \n",
    "We try both ReLU and Quadratic activation functions. \n",
    "\n",
    "### Training algorithm\n",
    "+ Stochastic Gradient Descent with batch size 64\n",
    "+ BCELoss\n",
    "+ Learning rate $\\gamma = 0.01$\n",
    "+ Stopping criterion: At least 10 epochs AND Training loss < 0.01 AND Rolling avg. of rel. change in training loss < 0.01 (window size 10). OR 1000 epochs.\n",
    "\n",
    "### Model Evaluation\n",
    "+ The model $M$ prediction is $\\mathbb{1}[M(x)>0.5]$. Test accuracy is the percentage of correct predictions over the test set.\n",
    "\n",
    "### Search algorithm\n",
    "\n",
    "We seek the number of training samples needed to reach a fixed test accuracy using a kind of bisection algorithm.\n",
    "1. Initial training run on 5000 samples.\n",
    "    + If test accuracy > 0.9, take half step towards 0 -> 2'500\n",
    "    + If test accuracy <= 0.9 take half step towards 10'000 -> 7500\n",
    "2. Repeat, next with quarter step.\n",
    "\n",
    "This is repeated $10$ times with different weight initialisations in case the test-accuracy curves are not monotonically increasing due to noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72649db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0dc21f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59c039ea",
   "metadata": {},
   "source": [
    "1. [Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?](https://arxiv.org/abs/2010.08515) Zhiyuan Li, Yi Zhang, Sanjeev Arora, 2021"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
