{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a91d765",
   "metadata": {},
   "source": [
    "# Sample Complexity Gap\n",
    "\n",
    "This notebook aims to demonstrate the stated sample complexity gap in **Why Are Convolutional Networks More Sample Efficient Than Fully-Connected Nets? by Zhiyuan Li, Yi Zhang and Sanjeev Arora** [1]. We set up an experiment in which we should see the gap as an increasing polynomial curve of degree less than two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95758862",
   "metadata": {},
   "source": [
    "## 1. Methods\n",
    "\n",
    "For a given input dimension $d$, we seek the number $|S_{tr}|$ of training samples needed for a model to reach $\\epsilon=0.9$ test accuracy. Then we plot the difference of training samples needed between a Convolutional Neural Network and a Fully Connected Neural Network for increasing values of $d$.\n",
    "\n",
    "### Data\n",
    "\n",
    "The inputs are $3\\times k \\times k$ RGB images for $k\\in \\mathbb{N}$, yielding input dimensions $d\\in \\{..., 192, 243, 300, 363, ...\\}$. We create full training set of 10000 images and a test set of 10'000 and we ask \"the first *how-many* training samples are needed to reach $90\\%$ test accuracy if we train until convergence\"? The training sets are constructed in the following manner.\n",
    "+ Entry-wise independent Gaussian (mean 0, standard deviation 1)\n",
    "\n",
    "We explore two different labelling functions \n",
    "\\begin{equation}\n",
    "h_1=\\mathbb{1}[\\sum_{i\\in R} x_i > \\sum_{i \\in G}x_i] \\quad\\mathrm{ and }\\quad h_2=\\mathbb{1}[\\sum_{i\\in R} x_i^2 > \\sum_{i \\in G}x_i^2].\n",
    "\\end{equation}\n",
    "\n",
    "### Models\n",
    "\n",
    "1. 2-layer CNN\n",
    "    + Convolution: One kernel per input channel of size 3x3, 10 output channels, stride size 1, and padding of 1, and bias\n",
    "    + Activation function\n",
    "    + Max pooling, kernel size 2x2, stride 2\n",
    "    + Fully connected layer (160 in, 1 out) with bias\n",
    "    + Sigmoid  \n",
    "2. 2-layer \"Dumb\" CNN. (DCNN)\n",
    "    + Convolution: One kernel per input channel of size 3x3, 2 output channels, stride size 1, and padding of 0, and bias\n",
    "    + Activation function\n",
    "    + Global average pooling, kernel size 8x8\n",
    "    + Fully connected layer (160 in, 1 out) with bias\n",
    "    + Sigmoid\n",
    "2. 2-layer FCNN \n",
    "    + Fully connected layer (192 in, 3072 out) with bias\n",
    "    + Activation function \n",
    "    + Fully connected layer (3072 in, 1 out) with bias\n",
    "    + Sigmoid\n",
    "    \n",
    "We try both ReLU and Quadratic activation functions. \n",
    "\n",
    "### Training algorithm\n",
    "+ Stochastic Gradient Descent with batch size 64\n",
    "+ BCELoss\n",
    "+ Learning rate $\\gamma = 0.01$\n",
    "+ Stopping criterion: At least 10 epochs AND Training loss < 0.01 AND Rolling avg. of rel. change in training loss < 0.01 (window size 10). OR 500 epochs.\n",
    "\n",
    "### Model Evaluation\n",
    "+ The model $M$ prediction is $\\mathbb{1}[M(x)>0.5]$. Test accuracy is the percentage of correct predictions over the test set.\n",
    "\n",
    "### Search algorithm\n",
    "\n",
    "We seek the number of training samples needed to reach a fixed test accuracy using a kind of bisection algorithm.\n",
    "1. Initial training run on 5000 samples.\n",
    "    + If test accuracy > 0.9, take half step towards 0 -> 2'500\n",
    "    + If test accuracy <= 0.9 take half step towards 10'000 -> 7500\n",
    "2. Reload initial weights and retrain. Make quarter step.\n",
    "\n",
    "This is repeated $10$ times with different weight initialisations in case the test-accuracy curves are not monotonically increasing due to noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd8342",
   "metadata": {},
   "source": [
    "## 2. Experiment\n",
    "\n",
    "Optional Arguments\n",
    "- -min/max (int) The min/max image side lengths. Default 4/14.\n",
    "- -f (str) The output .json file name (must contain an empty {}). Default week2_out.json\n",
    "- -e (int) The maximum number of epochs to run the programme for. Default 100.\n",
    "- -p (1/2) The p-norm used in the labelling function. Default 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bd80dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to run the experiment\n",
    "!python week2.py -min 4 -max 14 -f week2_out.json -e 20 -p 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb437ae",
   "metadata": {},
   "source": [
    "## 3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a355c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "file_path = 'week2_out.json'\n",
    "with open(file_path, 'r') as json_file:\n",
    "    test_acc = load(json_file)\n",
    "\n",
    "# Create sorted dictionary out of saved data\n",
    "new_dict = {}\n",
    "for key in test_acc.keys():\n",
    "    new_dict[int(key)]=test_acc[key]\n",
    "sorted_dict = dict(sorted(new_dict.items()))\n",
    "\n",
    "# Convert the dictionary to a format suitable for plotting\n",
    "x_values = np.sort([int(key) for key in sorted_dict.keys()])  # Convert keys to integers\n",
    "\n",
    "# Model names\n",
    "names = [\"CNN+ReLU\", \"CNN+Quad\", \"DCNN+ReLU\", \"DCNN+Quad\", \"FCNN+ReLU\", \"FCNN+Quad\"]\n",
    "\n",
    "# For every model, make line plot\n",
    "for i, name in enumerate(names):\n",
    "    \n",
    "    # The accuracy values for increasing number of samples\n",
    "    y_values = [value[i] for value in sorted_dict.values()]\n",
    "\n",
    "    # Create a line plot\n",
    "    plt.plot(x_values, y_values, marker='.', linestyle='-', label=name)\n",
    "    \n",
    "# Plot graphics\n",
    "plt.xlabel('Input dimension')\n",
    "plt.ylabel('Training set size')\n",
    "plt.title(f'Training set size required to reach {epsilon} test accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a6a808",
   "metadata": {},
   "source": [
    "1. [Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?](https://arxiv.org/abs/2010.08515) Zhiyuan Li, Yi Zhang, Sanjeev Arora, 2021"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
